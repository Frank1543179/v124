---
title: 'TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in (Un-)Supervised
  NLP'
abstract: While state-of-the-art NLP explainability (XAI) methods focus on explaining
  per-sample decisions in supervised end or probing tasks, this is insufficient to
  explain and quantify model knowledge transfer during (un-)supervised training. Thus,
  for TX-Ray, we modify the established computer vision explainability principle of
  ‘visualizing preferred inputs of neurons’ to make it usable for both NLP and for
  transfer analysis. This allows one to analyze, track and quantify how self- or supervised
  NLP models first build knowledge abstractions in pretraining (1), andthen transfer
  abstractions to a new domain (2), or adapt them during supervised finetuning (3)
  – see Fig. 1. TX-Ray expresses neurons as feature preference distributions to quantify
  fine-grained knowledge transfer or adaptation and guide human analysis. We find
  that, similar to Lottery Ticket based pruning, TX-Ray based pruning can improve
  test set generalization and that it can reveal how early stages of self-supervision
  automatically learn linguistic abstractions like parts-of-speech.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rethmeier20a
month: 0
tex_title: 'TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in (Un-)Supervised
  NLP'
firstpage: 440
lastpage: 449
page: 440-449
order: 440
cycles: false
bibtex_author: Rethmeier, Nils and Kumar Saxena, Vageesh and Augenstein, Isabelle
author:
- given: Nils
  family: Rethmeier
- given: Vageesh
  family: Kumar Saxena
- given: Isabelle
  family: Augenstein
date: 2020-08-27
address: 
container-title: Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
  (UAI)
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/rethmeier20a/rethmeier20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
