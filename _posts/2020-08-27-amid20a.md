---
title: Divergence-Based Motivation for Online EM and Combining Hidden Variable Models
abstract: Expectation-Maximization (EM) is a prominent approach for parameter estimation
  of hidden (aka latent) variable models. Given the full batch of data, EM forms an
  upper-bound of the negative log-likelihood of the model at each iteration and updates
  to the minimizer of this upper-bound. We first provide a “model level” interpretation
  of the EM upper-bound as a sum of relative entropy divergences to a set of singleton
  models induced by the batch of observations. Our alternative motivation unifies
  the “observation level” and the “model level” view of the EM. As a result, we formulate
  an online version of the EM algorithm by adding an analogous inertia term which
  is a relative entropy divergence to the old model. Our motivation is more widely
  applicable than the previous approaches and leads to simple online updates for mixture
  of exponential distributions, hidden Markov models, and the first known online update
  for Kalman filters. Additionally, the finite sample form of the inertia term lets
  us derive online updates when there is no closed-form solution. Finally, we extend
  the analysis to the distributed setting where we motivate a systematic way of combining
  multiple hidden variable models. Experimentally, we validate the results on synthetic
  as well as real-world datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: amid20a
month: 0
tex_title: Divergence-Based Motivation for Online EM and Combining Hidden Variable
  Models
firstpage: 81
lastpage: 90
page: 81-90
order: 81
cycles: false
bibtex_author: Amid, Ehsan and K. Warmuth, Manfred
author:
- given: Ehsan
  family: Amid
- given: Manfred
  family: K. Warmuth
date: 2020-08-27
address: 
container-title: Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
  (UAI)
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/amid20a/amid20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
