---
title: Towards Threshold Invariant Fair Classification
abstract: '{Effective machine learning models can automatically learn useful information
  from a large quantity of data and provide decisions in a high accuracy. These models
  may, however, lead to unfair predictions in certain sense among the population groups
  of interest, where the grouping is based on such sensitive attributes as race and
  gender. Various fairness definitions, such as demographic parity and equalized odds,
  were proposed in prior art to ensure that decisions guided by the machine learning
  models are equitable. Unfortunately, the "fair" model trained with these fairness
  definitions is threshold sensitive, i.e., the condition of fairness may no longer
  hold true when tuning the decision threshold. This paper introduces the notion of
  threshold invariant fairness, which enforces equitable performances across different
  groups independent of the decision threshold. To achieve this goal, this paper proposes
  to equalize the risk distributions among the groups via two approximation methods.
  Experimental results demonstrate that the proposed methodology is effective to alleviate
  the threshold sensitivity in machine learning models designed to achieve fairness.}'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen20b
month: 0
tex_title: "{Towards Threshold Invariant Fair Classification}"
firstpage: 560
lastpage: 569
page: 560-569
order: 560
cycles: false
bibtex_author: Chen, Mingliang and Wu, Min
author:
- given: Mingliang
  family: Chen
- given: Min
  family: Wu
date: 2020-08-27
address: 
container-title: "{Proceedings of the 36th Conference on Uncertainty in Artificial
  Intelligence (UAI)}"
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/chen20b/chen20b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
