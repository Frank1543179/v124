---
title: Batch norm with entropic regularization turns deterministic autoencoders into
  generative models
abstract: The variational autoencoder is a well defined deep generative model that
  utilizes an encoder-decoder framework where an encoding neural network outputs a
  non-deterministic code for reconstructing an input. The encoder achieves this by
  sampling from a distribution for every input, instead of outputting a deterministic
  code per input. The great advantage of this process is that it allows the use of
  the network as a generative model for sampling from the data distribution beyond
  provided samples for training. We show in this work that utilizing batch normalization
  as a source for non-determinism suffices to turn deterministic autoencoders into
  generative models on par with variational ones, so long as we add a suitable entropic
  regularization to the training objective.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ghose20a
month: 0
tex_title: Batch norm with entropic regularization turns deterministic autoencoders
  into generative models
firstpage: 1079
lastpage: 1088
page: 1079-1088
order: 1079
cycles: false
bibtex_author: Ghose, Amur and Rashwan, Abdullah and Poupart, Pascal
author:
- given: Amur
  family: Ghose
- given: Abdullah
  family: Rashwan
- given: Pascal
  family: Poupart
date: 2020-08-27
address: 
container-title: Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
  (UAI)
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/ghose20a/ghose20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v124/ghose20a/ghose20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
