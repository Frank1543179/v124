---
title: Amortized variance reduction for doubly stochastic objective
abstract: Approximate inference in complex probabilistic models such as deep Gaussian
  processes requires the optimisation of doubly stochastic objective functions. These
  objectives incorporate randomness both from mini-batch subsampling of the data and
  from Monte Carlo estimation of expectations. If the gradient variance is high, the
  stochastic optimisation problem becomes difficult with a slow rate of convergence.
  Control variates can be used to reduce the variance, but past approaches do not
  take into account how mini-batch stochasticity affects sampling stochasticity, resulting
  in sub-optimal variance reduction. We propose a new approach in which we use a recognition
  network to cheaply approximate the optimal control variate for each mini-batch,
  with no additional model gradient computations. We illustrate the properties of
  this proposal and test its performance on logistic regression and deep Gaussian
  processes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: boustati20a
month: 0
tex_title: Amortized variance reduction for doubly stochastic objective
firstpage: 61
lastpage: 70
page: 61-70
order: 61
cycles: false
bibtex_author: Boustati, Ayman and Vakili, Sattar and Hensman, James and John, ST
author:
- given: Ayman
  family: Boustati
- given: Sattar
  family: Vakili
- given: James
  family: Hensman
- given: ST
  family: John
date: 2020-08-27
address: 
container-title: Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
  (UAI)
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/boustati20a/boustati20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v124/boustati20a/boustati20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
