---
title: Learning Intrinsic Rewards as a Bi-Level Optimization Problem
abstract: 'We reinterpret the problem of finding intrinsic rewards in reinforcement
  learning (RL) as a bilevel optimization problem. Using this interpretation, we can
  make use of recent advancements in the hyperparameter optimization literature, mainly
  from Self-Tuning Networks (STN), to learn intrinsic rewards. To facilitate our methods,
  we introduces a new general conditioning layer: Conditional Layer Normalization
  (CLN). We evaluate our method on several continuous control benchmarks in the Mujoco
  physics simulator. On all of these benchmarks, the intrinsic rewards learned on
  the fly lead to higher final rewards.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: stadie20a
month: 0
tex_title: Learning Intrinsic Rewards as a Bi-Level Optimization Problem
firstpage: 111
lastpage: 120
page: 111-120
order: 111
cycles: false
bibtex_author: Stadie, Bradly and Zhang, Lunjun and Ba, Jimmy
author:
- given: Bradly
  family: Stadie
- given: Lunjun
  family: Zhang
- given: Jimmy
  family: Ba
date: 2020-08-27
address: 
container-title: Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
  (UAI)
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/stadie20a/stadie20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v124/stadie20a/stadie20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
