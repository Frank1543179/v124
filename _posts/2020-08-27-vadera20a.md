---
title: Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks
abstract: In this paper, we present a general framework for distilling expectations
  with respect to the Bayesian posterior distribution of a deep neural network classifier,
  extending prior work on the Bayesian Dark Knowledge framework.  The proposed framework
  takes as input "teacher" and "student" model architectures and a general posterior
  expectation of interest.  The distillation method performs an online compression
  of the selected posterior expectation using iteratively generated Monte Carlo samples.
  We focus on the posterior predictive distribution and expected entropy as distillation
  targets. We investigate several aspects of this framework including the impact of
  uncertainty and the choice of student model architecture. We study methods for student
  model architecture search from a speed-storage-accuracy perspective and evaluate
  down-stream tasks leveraging entropy distillation including uncertainty ranking
  and out-of-distribution detection.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vadera20a
month: 0
tex_title: Generalized Bayesian Posterior Expectation Distillation for Deep Neural
  Networks
firstpage: 719
lastpage: 728
page: 719-728
order: 719
cycles: false
bibtex_author: Vadera, Meet and Jalaian, Brian and Marlin, Benjamin
author:
- given: Meet
  family: Vadera
- given: Brian
  family: Jalaian
- given: Benjamin
  family: Marlin
date: 2020-08-27
address: 
container-title: Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
  (UAI)
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/vadera20a/vadera20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v124/vadera20a/vadera20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
